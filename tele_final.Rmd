---
title: "HW 4"
author: "Parth Pandit and Satvik Suneja"
date: "2022-10-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Downloading and Prepping the Data

```{r}
#Downloading and Prepping the Data
library(gmodels)
library(caret)
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
summary(tele)

#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL

# Deleting the column X
tele$X <- NULL

# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL

str(tele)
```

## Getting Data Ready for Analysis

```{r}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

telemm <- as.data.frame(model.matrix(~.-1,tele))

# Randomize the rows in the data (shuffling the rows)
set.seed(0)
tele_random <- telemm[sample(nrow(telemm)),]

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything 
tele_norm <- as.data.frame(lapply(tele_random, normalize))
str(tele_norm)
```


## Getting Train and Test Samples
```{r}
# Selects 10000 random rows for test data
test_set <- sample(1:nrow(tele_norm), 10000) 
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 

# Create a train set and test set
#First the predictors - all columns except the yyes column
train <- tele_norm[-test_set, -match("yyes",names(tele_norm))]
test <- tele_norm[test_set, -match("yyes",names(tele_norm))]

#Now the response (aka Labels) - only the yyes column
train_labels <- tele_norm[-test_set, "yyes"]
test_labels <- tele_norm[test_set, "yyes"]

```

#K-clustering
```{r}
##### Chapter 9: Clustering with k-means -------------------

#split off labels
tele_attributes <- tele_norm[, -match("yyes", names(tele_norm))]
tele_labels <- tele_norm[, match("yyes", names(tele_norm))]

tele_clusters <- kmeans(tele_attributes, 5)

# look at the size of the clusters
tele_clusters$size

# look at the cluster centers
tele_clusters$centers

## Improving model performance ----
# apply the cluster IDs to the original data frame

tele_clustered <- tele_attributes
tele_clustered$cluster <- tele_clusters$cluster
tele_clustered$yyes <- tele_labels

#see success rate for each cluster
aggregate(data = tele_clustered, yyes ~ cluster, mean)

kmeans.wss.k <- function(tele_attributes, k){
  km = kmeans(tele_attributes, k)
  return (km$tot.withinss)
}

kmeans.wss.k(tele_attributes, 5)
kmeans.wss.k(tele_attributes, 4)

kmeans.dis <- function(data_z, maxk){
  dis=(nrow(data_z)-1)*sum(apply(data_z,2,var))
  dis[2:maxk]=sapply (2:maxk, kmeans.wss.k, tele_attributes=tele_attributes)
  return(dis) }

maxk = 30
dis = kmeans.dis(tele_attributes, maxk)

plot(1:maxk, dis, type='b', xlab="Number of Clusters", ylab="Distortion", col="blue")
```
## K-Means Clustering Analysis
From the data frame of the success rate for each of the clusters, we can see that cluster 3 has the highest success rate. This means that it is worth to call every single person within this cluster so we do not need to run other models for prediction on whether or not to call for cluster 3. 


```{r}
#making each cluster its own data set
tele_cluster1 <- tele_clustered[tele_clustered$cluster == 1, ]
tele_cluster1$cluster <- NULL

tele_cluster2 <- tele_clustered[tele_clustered$cluster == 2, ]
tele_cluster2$cluster <- NULL

tele_cluster3 <- tele_clustered[tele_clustered$cluster == 3, ]
tele_cluster3$cluster <- NULL

tele_cluster4 <- tele_clustered[tele_clustered$cluster == 4, ]
tele_cluster4$cluster <- NULL

tele_cluster5 <- tele_clustered[tele_clustered$cluster == 5, ]
tele_cluster5$cluster <- NULL
```

## Logistic Regression Model
```{r}
logmodel = glm(yyes ~ age + jobblue.collar + jobentrepreneur + jobmanagement + jobretired + jobservices + jobstudent + jobtechnician + maritalsingle + educationuniversity.degree + educationunknown + defaultunknown + contacttelephone + monthaug + monthdec + monthjul + monthmar + monthmay + monthnov + monthoct + monthsep + day_of_weekmon + day_of_weekwed + poutcomesuccess + campaign, data = tele_norm, family = binomial)

summary(logmodel)

log_model_pred = predict(logmodel, newdata = tele_norm, type = "response")
yyes_logpred <- ifelse(log_model_pred < 0.5,0,1)
CrossTable(x = tele_norm$yyes, y = yyes_logpred, prop.chisq=FALSE)
confusionMatrix(as.factor(yyes_logpred), as.factor(tele_norm$yyes), positive = "1")

```


## Logistic Regression Model Predictions 
```{r}
#Cluster 1
log_cluster1 <- predict(logmodel, tele_cluster1)
yyes_cluster1 <- ifelse(log_cluster1 < 0.5,0,1)
CrossTable(x = tele_cluster1$yyes, y = yyes_cluster1, prop.chisq=FALSE)
confusionMatrix(as.factor(yyes_cluster1), as.factor(tele_cluster1$yyes), positive = "1")

#Cluster 2
log_cluster2 <- predict(logmodel, tele_cluster2)
yyes_cluster2 <- ifelse(log_cluster2 < 0.5,0,1)
CrossTable(x = tele_cluster2$yyes, y = yyes_cluster2, prop.chisq=FALSE)
confusionMatrix(as.factor(yyes_cluster2), as.factor(tele_cluster2$yyes), positive = "1")

#Cluster 4
log_cluster4 <- predict(logmodel, tele_cluster4)
yyes_cluster4 <- ifelse(log_cluster4 < 0.5,0,1)
CrossTable(x = tele_cluster4$yyes, y = yyes_cluster4, prop.chisq=FALSE)
confusionMatrix(as.factor(yyes_cluster4), as.factor(tele_cluster4$yyes), positive = "1")

#Cluster 5
log_cluster5 <- predict(logmodel, tele_cluster5)
yyes_cluster5 <- ifelse(log_cluster5 < 0.5,0,1)
CrossTable(x = tele_cluster5$yyes, y = yyes_cluster5, prop.chisq=FALSE)
confusionMatrix(as.factor(yyes_cluster5), as.factor(tele_cluster5$yyes), positive = "1")
```

## Log Model Analysis
After running the log model on each of the clusters, we see that it is the most accurate for cluster 4, with an accuracy of 96%. Clusters 4 and 5 are similar, but cluster 2 is significantly lower at 90%.



## ANN Model 
```{r}
# compared to the original minimum and maximum
summary(tele_norm$strength)


test_set <- sample(1:nrow(tele_norm), 10000) 

# create training and test data
tele_train <- tele_norm[-test_set, ]
tele_test <- tele_norm[test_set, ]

## Step 3: Training a model on the data ----
# train the neuralnet model
library(neuralnet)
tele_model2 <- neuralnet(yyes ~ .,
                            data = tele_train, hidden = 3)

# evaluate the results as we did before
model_results2 <- compute(tele_model2, tele_test)

prediction2 <- predict(tele_model2, tele_test)
yyes_pred2 <- ifelse(prediction2 < 0.5, 0, 1)
CrossTable(x = tele_test$yyes, y = yyes_pred2, prop.chisq=FALSE)

confusionMatrix(as.factor(yyes_pred2), as.factor(tele_test$yyes), positive = "1")
```

## ANN Model Predictions
```{r}
#Apply the ANN Model to each cluster
#Test Cluster 1
ANN_cluster1 <- predict(tele_model2, tele_cluster1)
ANN_yyes_cluster1 <- ifelse(ANN_cluster1 < 0.5,0,1)
CrossTable(x = tele_cluster1$yyes, y = ANN_yyes_cluster1, prop.chisq=FALSE)
confusionMatrix(as.factor(ANN_yyes_cluster1), as.factor(tele_cluster1$yyes), positive = "1")

#Test Cluster 2
ANN_cluster2 <- predict(tele_model2, tele_cluster2)
ANN_yyes_cluster2 <- ifelse(ANN_cluster2 < 0.5,0,1)
CrossTable(x = tele_cluster2$yyes, y = ANN_yyes_cluster2, prop.chisq=FALSE)
confusionMatrix(as.factor(ANN_yyes_cluster2), as.factor(tele_cluster2$yyes), positive = "1")

#Test Cluster 4
ANN_cluster4 <- predict(tele_model2, tele_cluster4)
ANN_yyes_cluster4 <- ifelse(ANN_cluster4 < 0.5,0,1)
CrossTable(x = tele_cluster4$yyes, y = ANN_yyes_cluster4, prop.chisq=FALSE)
confusionMatrix(as.factor(ANN_yyes_cluster4), as.factor(tele_cluster4$yyes), positive = "1")

#Test Cluster 5
ANN_cluster5 <- predict(tele_model2, tele_cluster5)
ANN_yyes_cluster5 <- ifelse(ANN_cluster5 < 0.5,0,1)
CrossTable(x = tele_cluster5$yyes, y = ANN_yyes_cluster5, prop.chisq=FALSE)
confusionMatrix(as.factor(ANN_yyes_cluster5), as.factor(tele_cluster5$yyes), positive = "1")

```

## KNN Model Predictions
```{r}
#Lets run the KNN command
library(class)
library(caret)
library(gmodels)
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 

# Create a train set and test set
#First the predictors
KNN_train <- tele_norm[-test_set, ]
KNN_train_labels <- tele_norm[-test_set, "yyes"]

##Test Clusters
#Test Cluster 1
cluster1labels <- tele_cluster1[, match("yyes", names(tele_norm))]
KNN_test_pred1 <- knn(train = KNN_train, test = tele_cluster1, cl = KNN_train_labels, k = 2)
CrossTable(x = cluster1labels, y =KNN_test_pred1, 
           prop.chisq=FALSE)
confusionMatrix(as.factor(KNN_test_pred1), as.factor(cluster1labels), positive = "1")

#Test Cluster 2
cluster2labels <- tele_cluster2[, match("yyes", names(tele_norm))]
KNN_test_pred2 <- knn(train = KNN_train, test = tele_cluster2, cl = KNN_train_labels, k = 2)
CrossTable(x = cluster2labels, y =KNN_test_pred2, 
           prop.chisq=FALSE)
confusionMatrix(as.factor(KNN_test_pred2), as.factor(cluster2labels), positive = "1")
#Test Cluster 4
cluster4labels <- tele_cluster4[, match("yyes", names(tele_norm))]
KNN_test_pred4 <- knn(train = KNN_train, test = tele_cluster4, cl = KNN_train_labels, k = 2)
CrossTable(x = cluster4labels, y =KNN_test_pred4, 
           prop.chisq=FALSE)
confusionMatrix(as.factor(KNN_test_pred4), as.factor(cluster4labels), positive = "1")
#Test Cluster 5
cluster5labels <- tele_cluster5[, match("yyes", names(tele_norm))]
KNN_test_pred5 <- knn(train = KNN_train, test = tele_cluster5, cl = KNN_train_labels, k = 2)
CrossTable(x = cluster5labels, y =KNN_test_pred5, 
           prop.chisq=FALSE)
confusionMatrix(as.factor(KNN_test_pred5), as.factor(cluster5labels), positive = "1")
```

## Combination Model Predictions
```{r}
#Voting model of all predictions
#Cluster 1
tele_cluster1$lgm <- as.numeric(yyes_cluster1)
tele_cluster1$ANN <- as.numeric(ANN_yyes_cluster1)
tele_cluster1$KNN <- as.numeric(KNN_test_pred1)
tele_cluster1$combined <- ifelse(tele_cluster1$lgm + tele_cluster1$ANN + tele_cluster1$KNN >=2, 1, 0)
CrossTable(x = cluster1labels, y = tele_cluster1$combined, 
           prop.chisq=FALSE)
confusionMatrix(as.factor(tele_cluster1$combined), as.factor(cluster1labels), positive = "1")
#Cluster 2
tele_cluster2$lgm <- as.numeric(yyes_cluster2)
tele_cluster2$ANN <- as.numeric(ANN_yyes_cluster2)
tele_cluster2$KNN <- as.numeric(KNN_test_pred2)
tele_cluster2$combined <- ifelse(tele_cluster2$lgm + tele_cluster2$ANN + tele_cluster2$KNN >=2, 1, 0)
CrossTable(x = cluster2labels, y = tele_cluster2$combined, 
           prop.chisq=FALSE)
confusionMatrix(as.factor(tele_cluster2$combined), as.factor(cluster2labels), positive = "1")
#Cluster 4
tele_cluster4$lgm <- as.numeric(yyes_cluster4)
tele_cluster4$ANN <- as.numeric(ANN_yyes_cluster4)
tele_cluster4$KNN <- as.numeric(KNN_test_pred4)
tele_cluster4$combined <- ifelse(tele_cluster4$lgm + tele_cluster4$ANN + tele_cluster4$KNN >=2, 1, 0)
CrossTable(x = cluster4labels, y = tele_cluster4$combined, 
           prop.chisq=FALSE)
confusionMatrix(as.factor(tele_cluster4$combined), as.factor(cluster4labels), positive = "1")
#Cluster 5
tele_cluster5$lgm <- as.numeric(yyes_cluster5)
tele_cluster5$ANN <- as.numeric(ANN_yyes_cluster5)
tele_cluster5$KNN <- as.numeric(KNN_test_pred5)
tele_cluster5$combined <- ifelse(tele_cluster5$lgm + tele_cluster5$ANN + tele_cluster5$KNN >=2, 1, 0)
CrossTable(x = cluster5labels, y = tele_cluster5$combined, 
           prop.chisq=FALSE)
confusionMatrix(as.factor(tele_cluster5$combined), as.factor(cluster5labels), positive = "1")
```

## SVM Model
```{r}
library(kernlab)

tele2 = tele[sample(1:nrow(tele)), ]

tele_train2 <- tele2[1:30900, ]
tele_test2  <- tele2[30900:41188, ]

tele_classifier <- ksvm(as.factor(y) ~ ., data = tele_train2,
                          kernel = "vanilladot")

tele_classifier


```

```{r}
tele2_predictions <- predict(tele_classifier, tele_test2)
head(tele2_predictions)

table(tele2_predictions, tele_test2$y)
agreement <- tele2_predictions == tele_test2$y
table(agreement)
prop.table(table(agreement))

```


# Decision Tree
```{r}
install.packages("C50")
```
```{r}
library(C50)

prop.table(table(tele_train2$y))
prop.table(table(tele_test2$y))

tele_dtmodel <- C5.0(as.factor(y) ~ ., data = tele)
plot(tele_dtmodel, subtree = 3)


```

```{r}

teledt_pred <- predict(tele_dtmodel, tele_test2)

# cross tabulation of predicted versus actual classes
library(gmodels)
CrossTable(tele_test2$y, teledt_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual pick up', 'predicted pick up'))

```

